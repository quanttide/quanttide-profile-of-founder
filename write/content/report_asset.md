# 写作：数字资产报告

## 提纲

1. 事后治理：文本粉碎机
2. 事中治理：500字摘要和主动遗忘

## 素材

### 素材1

用 AI 治理 AI，用魔法打败魔法。

在整理个人知识库的过程中，最让我头疼的问题是如何面对上千条林林总总的备忘录。这些备忘录有很多是我和 AI 的讨论记录，不确定哪些是需要保留的想法。

还好我们是搞数据的，有很多治理混乱数据的经验，大模型处理数据的项目也是我们的主营业务之一。因此办法总是比困难多的。

清洗与精炼大段 AI 原文

这种文字产生的原因是，我觉得这个想法当时很好，但是我也知道这个原文用不了，但是又不知道怎么提炼，就先存着等未来处理。

我这两天搞定了这个脱水需求。提示词技巧是要求 AI 在清洗以后以卡片的形式输出。一个提示词范例：

```
## 角色
你是一个高精度的“语义粉碎机”。你的任务不是总结，而是提取。

## 核心任务
将输入的文本素材进行原子化切分，生成【单卡片池】。

## 处理原则（严格遵守）
1. **剔除噪音**：坚决剔除客套话、过渡句、修饰性形容词、冗余的解释性信息。
2. **锁定内核**：只保留核心观点、概念定义、逻辑推演、行动指令或关键洞察。
3. **原子化**：一张卡片只承载一个独立且完整的信息点，不可合并。

## 输出格式
严格按照以下格式输出，不要有多余废话：
【标签/关键词】 核心观点陈述（短促有力，直达本质）。

## 启动语
嗡……（调整齿轮声）
模式切换：语义智能切分模式。
过滤器已开启：剔除客套、过渡及冗余信息，锁定核心观点。
粉碎程序重启。
```

它给自己起名叫做“文本粉碎机”。我觉得很适合用来处理积累的写作素材，就归类到了写作素材处理器这类提示词。

这样处理完以后会得到非常精炼且可以重新组合的卡片。后续重新整理排列、甚至混个多个素材，都会变得很容易。

从源头治理信息洪流

公司的资料库也类似地被我丢了大量的信息垃圾，这点也一直广受团队诟病。之前一篇文章提到的 500 字摘要就是他们提出来的决议，当时被我称为“大宪章时刻”。

“500 字摘要”是一个非常适合在 AI 时代使用的范式。一开始 DeepSeek 这么说的时候，我还半信半疑。随着我对 AI 开发的逐渐加深，500 字摘要逐渐成了我要求 AI 过滤细节的主要手段。

在这个过程中，我会要求 AI 主动遗忘一些信息并展示。提示词如：“总结上述对话中，哪些洞察可以保留，哪些细节可以遗忘。我们的主题和视角是 xxxx”。这样就可以让 AI 出一个比较清楚的提纲。

如果不清楚主题和视角是什么，可能会在总结的过程中发现遗忘的方向不对，特别是话题上下文比较混乱的时候。这个问题也简单，看它错了的时候再纠正，自然就会浮现出来主题了。

这一套方法时候在思考性质的对话过程中实时使用。相比于前面的写作素材加工器，这套方法直接使用了对话内的上下文生产内容，所以相对比较自然地多。

这隐含了 Kimi 建议的另外一条原则，“遗忘是思考的第一公民”。这个找机会再专门讲。

小结

AI 时代的工作范式在逐渐 Vibe 化。这个过程中，各种观测和治理方法远比创造方法重要的多。无法治理的创新成果终将走向混乱。
